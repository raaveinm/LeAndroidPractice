<?xml version="1.0" encoding="utf-8"?>
<resources xmlns:tools="http://schemas.android.com/tools">
    <string name="i_want_to_teleport">nächstes Layout</string>
    <string name="completed_sheesh">gemini.google.com</string>
    <string name="error_accessing_file">Fehler beim Zugriff auf die Datei</string>
    <string name="droplinear">Zum linearen Layout wechseln</string>
    <string name="dropframe">Zum Frame-Layout hinzufügen</string>
    <string name="toContinue">Start</string>

    <string name="introduction">Große Sprachmodelle (Large Language Models, LLMs) funktionieren durch ein komplexes Zusammenspiel von statistischer Modellierung, neuronalen Netzwerkarchitekturen und riesigen Datenmengen. Ihre Funktionsweise lässt sich grob in die folgenden Kernkomponenten zerlegen:</string>
    <string name="core_text">
        <b>1. Tokenisierung und Einbettung (Tokenization and Embedding):</b>
        Eingabetext wird zunächst in kleinere Einheiten zerlegt, die als Token bezeichnet werden.
        Diese Token können je nach Vokabular des jeweiligen Modells Wörter, Teilwörter oder sogar einzelne Zeichen darstellen.
        Jedes Token wird dann auf einen hochdimensionalen Vektor abgebildet, der als Einbettung (Embedding) bezeichnet wird.
        Diese Einbettung repräsentiert die semantische Bedeutung des Tokens und seine kontextuellen Beziehungen zu anderen Token, die während der Trainingsphase erlernt wurden.
        Der Einbettungsraum ist ein kontinuierlicher Vektorraum, in dem semantisch ähnliche Token nahe beieinander liegen (Clustering).
        \n\n<b>2. Transformer-Architektur (Transformer Architecture):</b>
        Die dominante Architektur, die modernen LLMs zugrunde liegt, ist der Transformer.
        Im Gegensatz zu rekurrenten neuronalen Netzen (RNNs), die Text sequentiell verarbeiten, verwendet der Transformer einen Mechanismus namens "Self-Attention" (Selbstaufmerksamkeit).
        Self-Attention ermöglicht es dem Modell, die Bedeutung verschiedener Token innerhalb einer Sequenz relativ zueinander zu gewichten, unabhängig von ihrer Position in der Sequenz.
        Dadurch kann das Modell weitreichende Abhängigkeiten und kontextuelle Nuancen im Text weitaus effektiver erfassen als RNNs.
        Die Transformer-Architektur besteht typischerweise aus mehreren gestapelten "Schichten" (Layers).
        Jede Schicht besteht aus zwei Unterschichten: einem Multi-Head-Self-Attention-Mechanismus und einem Feed-Forward-Netzwerk.
        \n\n<b>3. Multi-Head-Self-Attention:</b>
        Innerhalb jeder Self-Attention-Schicht werden die Eingabe-Einbettungen linear in drei separate Matrizen projiziert: Query (Q), Key (K) und Value (V).
        Aufmerksamkeits-Scores werden berechnet, indem das Punktprodukt der Query-Matrix mit der Transponierten der Key-Matrix gebildet wird.
        Diese Scores werden dann skaliert und mit einer Softmax-Funktion normalisiert, wodurch eine Wahrscheinlichkeitsverteilung entsteht, die die Aufmerksamkeitsgewichte darstellt, die jedem Token zugewiesen werden.
        Diese Gewichte bestimmen den Einfluss, den jedes Token auf die Repräsentation jedes anderen Tokens in der Sequenz hat.
        Der "Multi-Head"-Aspekt bezieht sich darauf, dass dieser Prozess mehrfach mit verschiedenen gelernten linearen Projektionen durchgeführt wird, wodurch das Modell verschiedene Aspekte der Beziehungen zwischen Token erfassen kann.
        Die Ausgaben jedes "Kopfes" werden dann verkettet und erneut linear projiziert.
        \n\n<b>4. Feed-Forward-Netzwerk (Feed-Forward Network):</b>
        Nach dem Self-Attention-Mechanismus wird die Repräsentation jedes Tokens durch ein positionsweises Feed-Forward-Netzwerk geleitet.
        Dieses Netzwerk besteht typischerweise aus zwei linearen Transformationen mit einer nichtlinearen Aktivierungsfunktion (z. B. ReLU oder GeLU) dazwischen.
        Diese Komponente fügt weitere Nichtlinearität hinzu und ermöglicht es dem Modell, komplexere Repräsentationen zu lernen.
        \n\n<b>5. Schichtstapelung und Residualverbindungen (Layer Stacking and Residual Connections):</b>
        Mehrere Transformer-Schichten werden sequentiell gestapelt.
        Jede Schicht verfeinert die Repräsentation der Token und baut auf dem Kontext und den Beziehungen auf, die von früheren Schichten erfasst wurden.
        Residualverbindungen (Addition der Eingabe einer Schicht zu ihrer Ausgabe) und Schichtnormalisierung werden eingesetzt, um die Trainingsstabilität und den Gradientenfluss zu verbessern und das Training sehr tiefer Netzwerke zu ermöglichen.
        \n\n<b>6. Vortraining und Feinabstimmung (Pre-training and Fine-tuning):</b>
        LLMs werden typischerweise in zwei Phasen trainiert.
        Das Vortraining (Pre-training) beinhaltet das Training des Modells auf einem riesigen Datensatz aus Text und Code, der oft aus dem Internet gesammelt wird.
        Das Modell lernt, das nächste Token in einer Sequenz vorherzusagen (autoregressive Sprachmodellierung) oder maskierte Token zu rekonstruieren (maskierte Sprachmodellierung).
        Diese Phase ermöglicht es dem Modell, allgemeine Sprachrepräsentationen und statistische Muster in den Daten zu lernen.
        Die Feinabstimmung (Fine-tuning) passt das vortrainierte Modell an eine bestimmte nachgelagerte Aufgabe an, wie z. B. Textgenerierung, Übersetzung oder Beantwortung von Fragen.
        Dies geschieht durch das Training des Modells auf einem kleineren, aufgabenspezifischen Datensatz.
        \n\n<b>7. Ausgabegenerierung (Output Generation):</b>
        Während der Inferenz (Textgenerierung) verarbeitet das Modell eine Eingabeaufforderung (Prompt, eine Folge von Token).
        Die Ausgabe des Modells ist eine Wahrscheinlichkeitsverteilung über sein gesamtes Vokabular, die die Wahrscheinlichkeit darstellt, mit der jedes Token das nächste Token in der Sequenz ist.
        Ein Dekodierungsalgorithmus (z. B. Greedy-Dekodierung, Beam Search, Top-k-Sampling) wird dann verwendet, um die wahrscheinlichste Folge von Token auszuwählen, um den Ausgabetext zu generieren.
        Das Modell generiert Text Token für Token, wobei jedes neue Token von den zuvor generierten Token und der Eingabeaufforderung abhängig gemacht wird.
        Die endgültige Ausgabe wird Token für Token aufgebaut, wobei der bis zu diesem Punkt erzeugte Kontext verwendet wird, um die gesamte nachfolgende Generierung zu steuern.
    </string>
    <string name="to_frame">zum Rahmenlayout</string>
    <string name="home">heim</string>
    <string name="back">zurück</string>
    <string name="to_linear">zum linearen</string>
    <string name="login" >einloggen</string>
    <string name="password">Passwort</string>
    <string name="username">Benutzername</string>
</resources>