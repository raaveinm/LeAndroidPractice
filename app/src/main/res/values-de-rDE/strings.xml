<?xml version="1.0" encoding="utf-8"?>
<resources xmlns:tools="http://schemas.android.com/tools">
    <string name="i_want_to_teleport">nächstes Layout</string>
    <string name="completed_sheesh">gemini.google.com</string>
    <string name="error_accessing_file">Fehler beim Zugriff auf die Datei</string>
    <string name="droplinear">Zum linearen Layout wechseln</string>
    <string name="dropframe">Zum Frame-Layout hinzufügen</string>
    <string name="toContinue">Start</string>

    <string name="introduction">Große Sprachmodelle (Large Language Models, LLMs) funktionieren durch ein komplexes Zusammenspiel von statistischer Modellierung, neuronalen Netzwerkarchitekturen und riesigen Datenmengen. Ihre Funktionsweise lässt sich grob in die folgenden Kernkomponenten zerlegen:</string>
    <string name="core_text">
        <b>1. Tokenisierung und Einbettung (Tokenization and Embedding):</b>
        Eingabetext wird zunächst in kleinere Einheiten zerlegt, die als Token bezeichnet werden.
        Diese Token können je nach Vokabular des jeweiligen Modells Wörter, Teilwörter oder sogar einzelne Zeichen darstellen.
        Jedes Token wird dann auf einen hochdimensionalen Vektor abgebildet, der als Einbettung (Embedding) bezeichnet wird.
        Diese Einbettung repräsentiert die semantische Bedeutung des Tokens und seine kontextuellen Beziehungen zu anderen Token, die während der Trainingsphase erlernt wurden.
        Der Einbettungsraum ist ein kontinuierlicher Vektorraum, in dem semantisch ähnliche Token nahe beieinander liegen (Clustering).
        \n\n<b>2. Transformer-Architektur (Transformer Architecture):</b>
        Die dominante Architektur, die modernen LLMs zugrunde liegt, ist der Transformer.
        Im Gegensatz zu rekurrenten neuronalen Netzen (RNNs), die Text sequentiell verarbeiten, verwendet der Transformer einen Mechanismus namens "Self-Attention" (Selbstaufmerksamkeit).
        Self-Attention ermöglicht es dem Modell, die Bedeutung verschiedener Token innerhalb einer Sequenz relativ zueinander zu gewichten, unabhängig von ihrer Position in der Sequenz.
        Dadurch kann das Modell weitreichende Abhängigkeiten und kontextuelle Nuancen im Text weitaus effektiver erfassen als RNNs.
        Die Transformer-Architektur besteht typischerweise aus mehreren gestapelten "Schichten" (Layers).
        Jede Schicht besteht aus zwei Unterschichten: einem Multi-Head-Self-Attention-Mechanismus und einem Feed-Forward-Netzwerk.
        \n\n<b>3. Multi-Head-Self-Attention:</b>
        Innerhalb jeder Self-Attention-Schicht werden die Eingabe-Einbettungen linear in drei separate Matrizen projiziert: Query (Q), Key (K) und Value (V).
        Aufmerksamkeits-Scores werden berechnet, indem das Punktprodukt der Query-Matrix mit der Transponierten der Key-Matrix gebildet wird.
        Diese Scores werden dann skaliert und mit einer Softmax-Funktion normalisiert, wodurch eine Wahrscheinlichkeitsverteilung entsteht, die die Aufmerksamkeitsgewichte darstellt, die jedem Token zugewiesen werden.
        Diese Gewichte bestimmen den Einfluss, den jedes Token auf die Repräsentation jedes anderen Tokens in der Sequenz hat.
        Der "Multi-Head"-Aspekt bezieht sich darauf, dass dieser Prozess mehrfach mit verschiedenen gelernten linearen Projektionen durchgeführt wird, wodurch das Modell verschiedene Aspekte der Beziehungen zwischen Token erfassen kann.
        Die Ausgaben jedes "Kopfes" werden dann verkettet und erneut linear projiziert.
        \n\n<b>4. Feed-Forward-Netzwerk (Feed-Forward Network):</b>
        Nach dem Self-Attention-Mechanismus wird die Repräsentation jedes Tokens durch ein positionsweises Feed-Forward-Netzwerk geleitet.
        Dieses Netzwerk besteht typischerweise aus zwei linearen Transformationen mit einer nichtlinearen Aktivierungsfunktion (z. B. ReLU oder GeLU) dazwischen.
        Diese Komponente fügt weitere Nichtlinearität hinzu und ermöglicht es dem Modell, komplexere Repräsentationen zu lernen.
        \n\n<b>5. Schichtstapelung und Residualverbindungen (Layer Stacking and Residual Connections):</b>
        Mehrere Transformer-Schichten werden sequentiell gestapelt.
        Jede Schicht verfeinert die Repräsentation der Token und baut auf dem Kontext und den Beziehungen auf, die von früheren Schichten erfasst wurden.
        Residualverbindungen (Addition der Eingabe einer Schicht zu ihrer Ausgabe) und Schichtnormalisierung werden eingesetzt, um die Trainingsstabilität und den Gradientenfluss zu verbessern und das Training sehr tiefer Netzwerke zu ermöglichen.
        \n\n<b>6. Vortraining und Feinabstimmung (Pre-training and Fine-tuning):</b>
        LLMs werden typischerweise in zwei Phasen trainiert.
        Das Vortraining (Pre-training) beinhaltet das Training des Modells auf einem riesigen Datensatz aus Text und Code, der oft aus dem Internet gesammelt wird.
        Das Modell lernt, das nächste Token in einer Sequenz vorherzusagen (autoregressive Sprachmodellierung) oder maskierte Token zu rekonstruieren (maskierte Sprachmodellierung).
        Diese Phase ermöglicht es dem Modell, allgemeine Sprachrepräsentationen und statistische Muster in den Daten zu lernen.
        Die Feinabstimmung (Fine-tuning) passt das vortrainierte Modell an eine bestimmte nachgelagerte Aufgabe an, wie z. B. Textgenerierung, Übersetzung oder Beantwortung von Fragen.
        Dies geschieht durch das Training des Modells auf einem kleineren, aufgabenspezifischen Datensatz.
        \n\n<b>7. Ausgabegenerierung (Output Generation):</b>
        Während der Inferenz (Textgenerierung) verarbeitet das Modell eine Eingabeaufforderung (Prompt, eine Folge von Token).
        Die Ausgabe des Modells ist eine Wahrscheinlichkeitsverteilung über sein gesamtes Vokabular, die die Wahrscheinlichkeit darstellt, mit der jedes Token das nächste Token in der Sequenz ist.
        Ein Dekodierungsalgorithmus (z. B. Greedy-Dekodierung, Beam Search, Top-k-Sampling) wird dann verwendet, um die wahrscheinlichste Folge von Token auszuwählen, um den Ausgabetext zu generieren.
        Das Modell generiert Text Token für Token, wobei jedes neue Token von den zuvor generierten Token und der Eingabeaufforderung abhängig gemacht wird.
        Die endgültige Ausgabe wird Token für Token aufgebaut, wobei der bis zu diesem Punkt erzeugte Kontext verwendet wird, um die gesamte nachfolgende Generierung zu steuern.
    </string>
    <string name="to_frame">zum Rahmenlayout</string>
    <string name="home">heim</string>
    <string name="back">zurück</string>
    <string name="to_linear">zum linearen</string>
    <string name="login" >einloggen</string>
    <string name="password">Passwort</string>
    <string name="username">Benutzername</string>
    <string name="extra_content_hello">Willkommen zu den zusätzlichen privaten Inhalten, die niemand gesehen hat</string>
    <string name="see_extra">Private Inhalte ansehen</string>
    <string name="extra_2">"Nix, der bezaubernde, pelzige und unbestreitbar wilde Begleiter von Kay Vess in Star Wars Outlaws, stammt aus einer faszinierenden, wenn auch etwas mysteriösen Ecke der Galaxie. Er ist ein Merqaal, eine Spezies, die im Mainstream-Universum von Star Wars nicht weit verbreitet ist, was ihn zu einer kleinen Besonderheit macht.     \\n\\nStell dir eine Kreatur vor, die die besten Eigenschaften eines treuen Hundes, einer verstohlenen Katze und vielleicht sogar eines schelmischen Affen vereint. Das beginnt, ein Bild von Nix zu zeichnen. Seine Geschichte ist größtenteils mit seiner Bindung zu Kay verbunden. Sie sind mehr als nur Partner; sie sind ein Team, das in den Feuern der Unterwelt geschmiedet wurde. Es heißt, dass Nix und Kay sich gefunden haben, und es wäre nicht zu weit hergeholt zu sagen, dass sie sich gegenseitig gerettet haben, wahrscheinlich während einer von Kays vielen Eskapaden oder brenzligen Situationen. Die Einzelheiten ihres ersten Treffens sind eine Geschichte, die noch nicht vollständig erzählt wurde, was ihre Mystik noch verstärkt.     \\n\\n<b>Nix</b> ist als Merqaal perfekt an das Leben eines Schurkenbegleiters angepasst. Er ist klein und wendig, kann sich in enge Räume zwängen, vorausspähen, Feinde ablenken und sogar Gegenstände holen. Diese Wendigkeit ist mit überraschender Stärke und Wildheit verbunden, wenn es nötig ist. Auch wenn er niedlich und kuschelig aussehen mag, sollte man Nix nicht unterschätzen. Er hat scharfe Krallen und Zähne und eine unerschütterliche Loyalität zu Kay, was ihn zu einem beeindruckenden Verbündeten in einem Kampf oder einem praktischen Helfer bei einem Raubüberfall macht.     \\n\\nDie Merqaal-Spezies selbst ist etwas geheimnisumwittert. Wir wissen nur wenig, aber wir wissen, dass sie von einer rauen Wüstenwelt stammen. Als Spezies sind sie sehr loyal. "</string>
    <string name="listItemExample" >Beispiel für „ListView öffnen“</string>
    <string name="recyclerViewText" />
    <string name="name" />
</resources>