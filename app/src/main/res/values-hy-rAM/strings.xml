<?xml version="1.0" encoding="utf-8"?>
<resources xmlns:tools="http://schemas.android.com/tools">
    <string name="i_want_to_teleport">Հաջորդ հատվածը</string>
    <string name="completed_sheesh">gemini.google.com</string>
    <string name="error_accessing_file">Չհաջողվեց մուտք գործել ֆայլ</string>
    <string name="droplinear">թողնել գծային դասավորությանը</string>
    <string name="dropframe">թողնել շրջանակի դասավորությունը</string>
    <string name="toContinue">Start</string>

    <string name="introduction">Մեծ Լեզվական Մոդելները (ՄԼՄ-ները) գործում են վիճակագրական մոդելավորման, նեյրոնային ցանցերի ճարտարապետությունների և զանգվածային տվյալների բազաների բարդ փոխազդեցության միջոցով: Դրանց ֆունկցիոնալությունը կարելի է մոտավորապես բաժանել հետևյալ հիմնական բաղադրիչների.</string>
    <string name="core_text">
    <b>1. Թոքենավորում և Ներկառուցում (Tokenization and Embedding):</b>
    Մուտքային տեքստը սկզբում տրոհվում է ավելի փոքր միավորների, որոնք կոչվում են թոքեններ:
    Այս թոքենները կարող են ներկայացնել բառեր, ենթաբառեր կամ նույնիսկ առանձին նիշեր՝ կախված մոդելի բառապաշարից:
    Այնուհետև յուրաքանչյուր թոքեն արտապատկերվում է բարձր չափողականության վեկտորի վրա, որը հայտնի է որպես ներկառուցում (embedding):
    Այս ներկառուցումը ներկայացնում է թոքենի իմաստային նշանակությունը և համատեքստային հարաբերությունները այլ թոքենների հետ, որոնք ուսուցանվում են մարզման փուլում:
    Ներկառուցման տարածությունը շարունակական վեկտորական տարածություն է, որտեղ իմաստային առումով նման թոքենները խմբավորվում են միասին:
    \n\n<b>2. Տրանսֆորմերի Ճարտարապետություն (Transformer Architecture):</b>
    Ժամանակակից ՄԼՄ-ների հիմքում ընկած գերիշխող ճարտարապետությունը Տրանսֆորմերն է:
    Ի տարբերություն ռեկուրենտ նեյրոնային ցանցերի (RNN), որոնք տեքստը մշակում են հաջորդաբար, Տրանսֆորմերն օգտագործում է «ինքնաուշադրություն» (self-attention) կոչվող մեխանիզմը:
    Ինքնաուշադրությունը թույլ է տալիս մոդելին կշռել հաջորդականության տարբեր թոքենների կարևորությունը միմյանց նկատմամբ՝ անկախ հաջորդականության մեջ նրանց դիրքից:
    Սա հնարավորություն է տալիս մոդելին շատ ավելի արդյունավետ կերպով որսալ տեքստի մեջ հեռահար կախվածություններն ու համատեքստային նրբությունները, քան RNN-ները:
    Տրանսֆորմերի ճարտարապետությունը սովորաբար բաղկացած է բազմաթիվ շերտերից (layers):
    Յուրաքանչյուր շերտ բաղկացած է երկու ենթաշերտերից՝ բազմագլուխ ինքնաուշադրության մեխանիզմից (multi-head self-attention) և առաջընթաց կապերով նեյրոնային ցանցից (feed-forward neural network):
    \n\n<b>3. Բազմագլուխ Ինքնաուշադրություն (Multi-Head Self-Attention):</b>
    Յուրաքանչյուր ինքնաուշադրության շերտի ներսում մուտքային ներկառուցումները գծային կերպով պրոյեկտվում են երեք առանձին մատրիցների վրա՝ Հարցում (Query, Q), Բանալի (Key, K) և Արժեք (Value, V):
    Ուշադրության գնահատականները հաշվարկվում են՝ վերցնելով Հարցման մատրիցի կետային արտադրյալը Բանալու մատրիցի տրանսպոնացվածի հետ:
    Այս գնահատականներն այնուհետև մասշտաբավորվում և նորմալացվում են՝ օգտագործելով softmax ֆունկցիան, որը ստեղծում է հավանականությունների բաշխում, որը ներկայացնում է յուրաքանչյուր թոքենին հատկացված ուշադրության կշիռները:
    Այս կշիռները որոշում են, թե յուրաքանչյուր թոքեն ինչ ազդեցություն ունի հաջորդականության մյուս բոլոր թոքենների ներկայացման վրա:
    «Բազմագլուխ» ասպեկտը վերաբերում է այս գործընթացը մի քանի անգամ կատարելուն՝ տարբեր ուսուցանված գծային պրոյեկցիաներով, ինչը թույլ է տալիս մոդելին ֆիքսել թոքենների միջև հարաբերությունների տարբեր կողմերը:
    Այնուհետև յուրաքանչյուր «գլխի» արդյունքները միավորվում են և կրկին գծային պրոյեկտվում:
    \n\n<b>4. Առաջընթաց կապերով Ցանց (Feed-Forward Network):</b>
    Ինքնաուշադրության մեխանիզմից հետո յուրաքանչյուր թոքենի ներկայացումն անցնում է դիրքային առաջընթաց կապերով նեյրոնային ցանցի միջով:
    Այս ցանցը սովորաբար բաղկացած է երկու գծային փոխակերպումներից՝ դրանց միջև ոչ գծային ակտիվացման ֆունկցիայով (օրինակ՝ ReLU կամ GeLU):
    Այս բաղադրիչը ավելացնում է լրացուցիչ ոչ գծայնություն և թույլ է տալիս մոդելին սովորել ավելի բարդ ներկայացումներ:
    \n\n<b>5. Շերտերի Կուտակում և Մնացորդային Կապեր (Layer Stacking and Residual Connections):</b>
    Բազմաթիվ Տրանսֆորմերի շերտեր հաջորդաբար կուտակվում են:
    Յուրաքանչյուր շերտ կատարելագործում է թոքենների ներկայացումը՝ հիմնվելով նախորդ շերտերի կողմից ֆիքսված համատեքստի և հարաբերությունների վրա:
    Մնացորդային կապերը (շերտի մուտքը գումարվում է նրա ելքին) և շերտի նորմալացումն օգտագործվում են՝ բարելավելու մարզման կայունությունը և գրադիենտի հոսքը՝ թույլ տալով մարզել շատ խորը ցանցեր:
    \n\n<b>6. Նախնական Մարզում և Ճշգրտում (Pre-training and Fine-tuning):</b>
    ՄԼՄ-ները սովորաբար մարզվում են երկու փուլով:
    Նախնական մարզումը (Pre-training) ներառում է մոդելի մարզումը տեքստի և կոդի հսկայական տվյալների բազայի վրա, որը հաճախ քաղված է ինտերնետից:
    Մոդելը սովորում է կանխատեսել հաջորդ թոքենը հաջորդականության մեջ (ավտոռեգրեսիվ լեզվի մոդելավորում) կամ վերականգնել դիմակավորված թոքենները (դիմակավորված լեզվի մոդելավորում):
    Այս փուլը թույլ է տալիս մոդելին սովորել ընդհանուր նշանակության լեզվական ներկայացումներ և տվյալների մեջ վիճ
    </string>
    <string name="to_frame">շրջանակի դասավորությունը</string>
    <string name="home">տուն</string>
    <string name="back">ետ</string>
    <string name="to_linear">դեպի գծային դասավորություն</string>
</resources>