<?xml version="1.0" encoding="utf-8"?>
<resources xmlns:tools="http://schemas.android.com/tools">
    <string name="i_want_to_teleport">Հաջորդ հատվածը</string>
    <string name="completed_sheesh">gemini.google.com</string>
    <string name="error_accessing_file">Չհաջողվեց մուտք գործել ֆայլ</string>
    <string name="droplinear">թողնել գծային դասավորությանը</string>
    <string name="dropframe">թողնել շրջանակի դասավորությունը</string>
    <string name="toContinue">Start</string>

    <string name="introduction">Մեծ Լեզվական Մոդելները (ՄԼՄ-ները) գործում են վիճակագրական մոդելավորման, նեյրոնային ցանցերի ճարտարապետությունների և զանգվածային տվյալների բազաների բարդ փոխազդեցության միջոցով: Դրանց ֆունկցիոնալությունը կարելի է մոտավորապես բաժանել հետևյալ հիմնական բաղադրիչների.</string>
    <string name="core_text">1. **Թոքենավորում և Ներկառուցում (Embedding):** Մուտքային տեքստը սկզբում տրոհվում է ավելի փոքր միավորների, որոնք կոչվում են թոքեններ: Այս թոքենները կարող են ներկայացնել բառեր, ենթաբառեր կամ նույնիսկ առանձին նիշեր՝ կախված մոդելի բառապաշարից: Այնուհետև յուրաքանչյուր թոքեն արտապատկերվում է բարձր չափողականության վեկտորի վրա, որը հայտնի է որպես ներկառուցում (embedding): Այս ներկառուցումը ներկայացնում է թոքենի իմաստային նշանակությունը և համատեքստային հարաբերությունները այլ թոքենների հետ, որոնք ուսուցանվում են մարզման փուլում: Ներկառուցման տարածությունը շարունակական վեկտորական տարածություն է, որտեղ իմաստային առումով նման թոքենները խմբավորվում են միասին:2. **Տրանսֆորմերի Ճարտարապետություն:** Ժամանակակից ՄԼՄ-ների հիմքում ընկած գերիշխող ճարտարապետությունը Տրանսֆորմերն է: Ի տարբերություն ռեկուրենտ նեյրոնային ցանցերի (RNN), որոնք տեքստը մշակում են հաջորդաբար, Տրանսֆորմերն օգտագործում է «ինքնաուշադրություն» (self-attention) կոչվող մեխանիզմը: Ինքնաուշադրությունը թույլ է տալիս մոդելին կշռել հաջորդականության տարբեր թոքենների կարևորությունը միմյանց նկատմամբ՝ անկախ հաջորդականության մեջ նրանց դիրքից: Սա հնարավորություն է տալիս մոդելին շատ ավելի արդյունավետ կերպով որսալ տեքստի մեջ հեռահար կախվածություններն ու համատեքստային նրբությունները, քան RNN-ները: Տրանսֆորմերի ճարտարապետությունը սովորաբար բաղկացած է բազմաթիվ շերտերից (layers): Յուրաքանչյուր շերտ բաղկացած է երկու ենթաշերտերից՝ բազմագլուխ ինքնաուշադրության մեխանիզմից (multi-head self-attention) և առաջընթաց կապերով նեյրոնային ցանցից (feed-forward neural network):3. **Բազմագլուխ Ինքնաուշադրություն (Multi-Head Self-Attention):** Յուրաքանչյուր ինքնաուշադրության շերտի ներսում մուտքային ներկառուցումները գծային կերպով պրոյեկտվում են երեք առանձին մատրիցների վրա՝ Հարցում (Query, Q), Բանալի (Key, K) և Արժեք (Value, V): Ուշադրության գնահատականները հաշվարկվում են՝ վերցնելով Հարցման մատրիցի կետային արտադրյալը Բանալու մատրիցի տրանսպոնացվածի հետ: Այս գնահատականներն այնուհետև մասշտաբավորվում և նորմալացվում են՝ օգտագործելով softmax ֆունկցիան, որը ստեղծում է հավանականությունների բաշխում, որը ներկայացնում է յուրաքանչյուր թոքենին հատկացված ուշադրության կշիռները: Այս կշիռները որոշում են, թե յուրաքանչյուր թոքեն ինչ ազդեցություն ունի հաջորդականության մյուս բոլոր թոքենների ներկայացման վրա: «Բազմագլուխ» ասպեկտը վերաբերում է այս գործընթացը մի քանի անգամ կատարելուն՝ տարբեր ուսուցանված գծային պրոյեկցիաներով, ինչը թույլ է տալիս մոդելին ֆիքսել թոքենների միջև հարաբերությունների տարբեր կողմերը: Այնուհետև յուրաքանչյուր «գլխի» արդյունքները միավորվում են և կրկին գծային պրոյեկտվում:4. **Առաջընթաց կապերով Ցանց (Feed-Forward Network):** Ինքնաուշադրության մեխանիզմից հետո յուրաքանչյուր թոքենի ներկայացումն անցնում է դիրքային առաջընթաց կապերով նեյրոնային ցանցի միջով: Այս ցանցը սովորաբար բաղկացած է երկու գծային փոխակերպումներից՝ դրանց միջև ոչ գծային ակտիվացման ֆունկցիայով (օրինակ՝ ReLU կամ GeLU): Այս բաղադրիչը ավելացնում է լրացուցիչ ոչ գծայնություն և թույլ է տալիս մոդելին սովորել ավելի բարդ ներկայացումներ:5. **Շերտերի Կուտակում և Մնացորդային Կապեր (Residual Connections):** Բազմաթիվ Տրանսֆորմերի շերտեր հաջորդաբար կուտակվում են: Յուրաքանչյուր շերտ կատարելագործում է թոքենների ներկայացումը՝ հիմնվելով նախորդ շերտերի կողմից ֆիքսված համատեքստի և հարաբերությունների վրա: Մնացորդային կապերը (շերտի մուտքը գումարվում է նրա ելքին) և շերտի նորմալացումն օգտագործվում են՝ բարելավելու մարզման կայունությունը և գրադիենտի հոսքը՝ թույլ տալով մարզել շատ խորը ցանցեր:6. **Նախնական Մարզում և Ճշգրտում (Pre-training and Fine-tuning):** ՄԼՄ-ները սովորաբար մարզվում են երկու փուլով: Նախնական մարզումը (Pre-training) ներառում է մոդելի մարզումը տեքստի և կոդի հսկայական տվյալների բազայի վրա, որը հաճախ քաղված է ինտերնետից: Մոդելը սովորում է կանխատեսել հաջորդ թոքենը հաջորդականության մեջ (ավտոռեգրեսիվ լեզվի մոդելավորում) կամ վերականգնել դիմակավորված թոքենները (դիմակավորված լեզվի մոդելավորում): Այս փուլը թույլ է տալիս մոդելին սովորել ընդհանուր նշանակության լեզվական ներկայացումներ և տվյալների մեջ վիճակագրական օրինաչափություններ: Ճշգրտումը (Fine-tuning) հարմարեցնում է նախապես մարզված մոդելը կոնկրետ առաջադրանքի, ինչպիսիք են տեքստի գեներացումը, թարգմանությունը կամ հարցերի պատասխանումը: Դա արվում է մոդելը մարզելով ավելի փոքր, առաջադրանքին հատուկ տվյալների բազայի վրա:7. **Ելքի Գեներացում (Output Generation):** Ելքագրման (տեքստի գեներացիայի) ընթացքում մոդելը մշակում է մուտքային հուշում (prompt, թոքենների հաջորդականություն): Մոդելի ելքը հավանականությունների բաշխում է իր ամբողջ բառապաշարի վրա, որը ներկայացնում է յուրաքանչյուր թոքենի հավանականությունը՝ որպես հաջորդականության հաջորդ թոքեն: Այնուհետև օգտագործվում է ապակոդավորման ալգորիթմ (օրինակ՝ greedy decoding, beam search, top-k sampling)՝ ընտրելու թոքենների ամենահավանական հաջորդականությունը՝ ելքային տեքստը գեներացնելու համար: Մոդելը տեքստը գեներացնում է մեկ թոքեն մեկ անգամ՝ պայմանավորելով յուրաքանչյուր նոր թոքենը նախկինում գեներացված թոքեններով և մուտքային հուշումով: Վերջնական ելքը կառուցվում է թոքեն առ թոքեն՝ օգտագործելով մինչ այդ պահը ստեղծված համատեքստը՝ բոլոր հետագա գեներացիան տեղեկացնելու համար։</string>
</resources>